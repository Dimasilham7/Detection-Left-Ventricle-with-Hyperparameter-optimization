{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unet_vgg16_jt_0.001_100_256_yes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUk3Ei1cr6fX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "9912c67b-7d0a-44eb-ad9b-155c94240fb8"
      },
      "source": [
        "# Install required libs  \n",
        "# NOTE: Run this one code, then restart this runtime and run again for next all... (PENTING!!!) \n",
        "\n",
        "### please update Albumentations to version>=0.3.0 for `Lambda` transform support\n",
        "!pip install -U --pre segmentation-models --user"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: segmentation-models in /root/.local/lib/python3.6/site-packages (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: image-classifiers==1.0.0 in /root/.local/lib/python3.6/site-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: efficientnet==1.0.0 in /root/.local/lib/python3.6/site-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LamlioVatAJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "88817a7f-8cb1-468c-f982-ee0a56f74f05"
      },
      "source": [
        "## Imports libs\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import cv2\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuPgm7m2tCeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "36003dfd-f30c-4fd5-b049-777f4f7a02e0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0u2JJtItLtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_link = '/content/drive/My Drive/'\n",
        "my_link = drive_link+'SKRIPSI/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuqVrsQ5taPH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "93e28abd-a1ca-4a14-e747-7590aa7a710e"
      },
      "source": [
        "DATA_DIR = my_link+'dataset/'\n",
        "\n",
        "# load repo with data if it is not exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "  print('Dataset not exist...')\n",
        "else:\n",
        "  print('Dataset Exist...')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Exist...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_sDMlInthqW",
        "colab_type": "text"
      },
      "source": [
        "### Data Generator\n",
        "###### ``Data Generator adalah kelas generator dataset yang dibungkus per batch sesuai dengan ukuran batch, ``\n",
        "###### ``misal (batch_size, pixel_width, pixel_height, channel_size) = (8,128,128,3)``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrSh3vGxtcBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGen(keras.utils.Sequence):\n",
        "    def __init__(self, ids, path, batch_size=8, image_size=256, augmentation=None, preprocessing=None): \n",
        "        self.ids = ids\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "        \n",
        "    def __load__(self,id_name):\n",
        "        ## Path\n",
        "        image_path = os.path.join(self.path, \"images\", id_name) + \".png\"\n",
        "        mask_path = os.path.join(self.path, \"masks\", id_name) + \".png\"\n",
        "        \n",
        "        ## Reading Image\n",
        "        image = cv2.imread(image_path, 1)\n",
        "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
        "        \n",
        "        ## Reading Masks\n",
        "        mask = cv2.imread(mask_path, -1)\n",
        "        mask = cv2.resize(mask, (self.image_size, self.image_size))\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "            \n",
        "        ## Normalizaing \n",
        "        image = image/255.0\n",
        "        mask = mask/255.0\n",
        "        \n",
        "        return image, mask\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ## Jumlah Batch Terakhir Disesuaikan Sisa Data Yang Ada\n",
        "        if(index+1)*self.batch_size > len(self.ids):\n",
        "            self.batch_size = len(self.ids) - index*self.batch_size\n",
        "        \n",
        "        files_batch = self.ids[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        \n",
        "        image = []\n",
        "        mask  = []\n",
        "        \n",
        "        ## Load Data Dengan Fungsi __load__\n",
        "        for id_name in files_batch:\n",
        "            _img, _mask = self.__load__(id_name)  \n",
        "            image.append(_img)\n",
        "            mask.append(_mask)\n",
        "            \n",
        "        image = np.array(image)\n",
        "        mask  = np.array(mask)\n",
        "\n",
        "        return image, mask\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "    \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.ids)/float(self.batch_size)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAUiPFrCtkQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "197d8a27-a89f-428b-cfe8-8a979ef1be96"
      },
      "source": [
        "# Data jantung generate ids\n",
        "\n",
        "train_path = DATA_DIR\n",
        "jantung_path = '/content/drive/My Drive/SKRIPSI/dataset/jantung_idx.txt' # daftar nama data jantung ada disini\n",
        "load_jt_ids = []\n",
        "\n",
        "# Load idx dataset jantung\n",
        "with open(jantung_path, 'rb') as handle: \n",
        "    load_jt_ids = [str(line.rstrip())[2:-1] for line in handle]\n",
        "\n",
        "# Membagi dataset menjadi train, valid, test = 70%, 20%, 10% \n",
        "train_jt_ids = load_jt_ids[:int(5011*0.70)+1]\n",
        "valid_jt_ids = load_jt_ids[int(5011*0.70)+1:-int(5011*0.10)]\n",
        "test_jt_ids = load_jt_ids[-int(5011*0.10):]\n",
        "\n",
        "print('Jumlah data jantung (train, valid, test) = (',len(train_jt_ids), ',', len(valid_jt_ids), ',', len(test_jt_ids),').')\n",
        "print('Total dataset: ',len(train_jt_ids)+len(valid_jt_ids)+len(test_jt_ids))\n",
        "print('Contoh data:',load_jt_ids[-1])\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jumlah data jantung (train, valid, test) = ( 3508 , 1002 , 501 ).\n",
            "Total dataset:  5011\n",
            "Contoh data: sol_033_z_pos_010_t_pos_020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaJ19qUMtxNo",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_QKcVPBtuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############____________________________________________________________############\n",
        "\n",
        "# Definisikan model dan parameter yang diinginkan... \n",
        "# Definisi parameter ini akan digenerate otomatis oleh fungsi-fungsi dibawahnya\n",
        "\n",
        "model_path = 'unet_vgg16_jt_0.001_100_256_yes'\n",
        "\n",
        "############____________________________________________________________############"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPwLMQzIuCSL",
        "colab_type": "text"
      },
      "source": [
        "Definisikan model_path: ``{decoder}_{encoder}_jt_{learning_rate}_{epochs}_{image_size}_{pretrained_imagenet?}``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyiTIfSjuCO-",
        "colab_type": "text"
      },
      "source": [
        "**Berikut definisi encoder di sm-model:**\n",
        "##### ``VGG``\t: 'vgg16', 'vgg19'\n",
        "##### ``ResNet``\t: 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152'\n",
        "##### ``SENet154``\t: \t'senet154'\n",
        "##### ``DenseNet``\t: \t'densenet121', 'densenet169', 'densenet201'\n",
        "##### ``Inception``\t: \t'inceptionv3', 'inceptionresnetv2'\n",
        "##### ``MobileNet``\t: \t'mobilenet', 'mobilenetv2'\n",
        "##### ``EfficientNet``\t: \t'efficientnetb0', 'efficientnetb1', 'efficientnetb2', 'efficientnetb3', 'efficientnetb4', 'efficientnetb5' efficientnetb6', efficientnetb7'\n",
        "\n",
        "**Berikut definisi decoder di sm-model:**\n",
        "- unet\n",
        "- linknet\n",
        "- pspnet\n",
        "- fpn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0oX7FraETZA",
        "colab_type": "text"
      },
      "source": [
        "#### Fungsi dan Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2qlazo7t_1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Posisi direktori model\n",
        "save_dir =  [\n",
        "              '/content/drive/My Drive/SKRIPSI/models/',\n",
        "            ]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4DYhjnJEXUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fungsi untuk mengolah history\n",
        "import json, codecs, pickle\n",
        "\n",
        "def saveHist(path, history):\n",
        "    with open(path, 'wb') as handle: # saving the history of the model\n",
        "        pickle.dump(history, handle)\n",
        "\n",
        "def loadHist(path):\n",
        "    n = {} # set history to empty\n",
        "    if os.path.exists(path): # reload history if it exists\n",
        "        with open(path, 'rb') as handle: # loading old history \n",
        "            n = pickle.load(handle)\n",
        "    return n\n",
        "\n",
        "def appendHist(h1, h2):\n",
        "    if h1 == {}:\n",
        "        return h2\n",
        "    else:\n",
        "        dest = {}\n",
        "        for key, value in h1.items():\n",
        "            dest[key] = value + h2[key]\n",
        "        return dest"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1CIRmeVEZqo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e1c18276-b89a-44e7-af18-d41163fa5db0"
      },
      "source": [
        "import segmentation_models as sm\n",
        "\n",
        "# Mengambil Model Yang Sudah Ada\n",
        "def take_model(model_path):\n",
        "    print('Melanjutkan: ',model_path)\n",
        "    md_ = model_path.split('_')\n",
        "    lr = float(md_[3])\n",
        "    epochs = int(md_[4])\n",
        "    image_size = int(md_[5])\n",
        "    pretrained = md_[6]\n",
        "    batch_size = 8\n",
        "    print('lr:',lr,', epochs:',epochs,', im_size:',image_size,', prt: '+pretrained)\n",
        "    # optim, loss, metric harus sesuai dengan model yang ditraining sebelumnya\n",
        "    optim = keras.optimizers.Adam(lr)\n",
        "    total_loss = sm.losses.JaccardLoss()\n",
        "    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5),'accuracy']\n",
        "\n",
        "    model = keras.models.load_model(save_dir[0]+model_path+'/'+model_path+'.h5', custom_objects={'jaccard_loss': total_loss, 'iou_score':metrics[0], 'f1-score':metrics[1]})\n",
        "\n",
        "    return model, lr, epochs, image_size, pretrained, batch_size, metrics, total_loss, optim\n",
        "\n",
        "# Mengatur Dataset\n",
        "def set_data(train_ids,test_ids,valid_ids):\n",
        "    train_gen = DataGen(train_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
        "    test_gen = DataGen(test_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
        "    valid_gen = DataGen(valid_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
        "\n",
        "    train_steps = len(train_ids)//batch_size\n",
        "    test_steps = len(test_ids)//batch_size\n",
        "    valid_steps = len(valid_ids)//batch_size\n",
        "\n",
        "    return train_gen,test_gen,valid_gen,train_steps,test_steps,valid_steps\n",
        "\n",
        "# Melanjutkan Training\n",
        "def cont_training(model):\n",
        "    print('Lanjutkan Training... '+model.name)   \n",
        "    path = save_dir[0]+model.name+'/'+model.name\n",
        "    old_hist = loadHist(path+'_history.bin')\n",
        "    initial_epoch = len(old_hist['loss'])\n",
        "    print('Dari epochs',initial_epoch)\n",
        "    if initial_epoch>=500 or initial_epoch==50 or initial_epoch==250:\n",
        "        print(\"Batal Training, Model Sudah Memenuhi Epochs \"+str(initial_epoch))\n",
        "    else:\n",
        "        # define callbacks for learning rate scheduling and best checkpoints saving\n",
        "        callbacks = [\n",
        "            keras.callbacks.ModelCheckpoint(path+'_best_weights.h5', monitor='val_iou_score', save_weights_only=True, save_best_only=True, mode='max'),\n",
        "        ]\n",
        "        saveHist(path+'_history_temp_'+str(initial_epoch)+'.bin', old_hist)\n",
        "        model.save(path+'_temp_'+str(initial_epoch)+'.h5')\n",
        "        model.fit_generator(train_gen, \n",
        "                            validation_data=valid_gen, \n",
        "                            steps_per_epoch=train_steps, \n",
        "                            validation_steps=valid_steps, \n",
        "                            epochs=initial_epoch+epochs,\n",
        "                            initial_epoch=initial_epoch,\n",
        "                            callbacks=callbacks)\n",
        "        print('Menyimpan Model...')    \n",
        "        model.save(path+'.h5')\n",
        "        history = model.history\n",
        "        model.history.history = appendHist(old_hist, history.history)\n",
        "        saveHist(path+'_history.bin', model.history.history)\n",
        "        if initial_epoch>=400:\n",
        "            train_vis(model.history.history, path, doing=\"save\")\n",
        "        print('Berhasil Menyimpan Model')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Segmentation Models: using `keras` framework.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOS3IQG8FPLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Membuat Model SM Baru\n",
        "def take_sm(model_path):\n",
        "    print('Memulai SM-Model: ',model_path)\n",
        "    try:\n",
        "        # apakah model sudah pernah dibuat?\n",
        "        os.mkdir(save_dir[0]+model_path)\n",
        "    except:\n",
        "        print('Overwrite '+model_path)\n",
        "    md_ = model_path.split('_')\n",
        "    lr = float(md_[3])\n",
        "    epochs = int(md_[4])\n",
        "    image_size = int(md_[5])\n",
        "    pretrained = md_[6]\n",
        "    batch_size = 8\n",
        "    print('lr:',lr,', epochs:',epochs,', im_size:',image_size,', prt: '+pretrained)\n",
        "    # optimizer manual setting \n",
        "    optim = keras.optimizers.Adam(lr)\n",
        "    # loss function manual setting\n",
        "    total_loss = sm.losses.JaccardLoss()\n",
        "    # metric evaluation manual setting\n",
        "    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), 'accuracy']\n",
        "    if pretrained=='yes': encoder_weights = 'imagenet'\n",
        "    else: encoder_weights = None\n",
        "    if md_[0] is 'unet':\n",
        "        model = sm.Unet(md_[1], classes=1, activation='sigmoid', input_shape=(image_size,image_size,3), encoder_weights=encoder_weights) \n",
        "    elif md_[0] is 'linknet':\n",
        "        model = sm.Linknet(md_[1], classes=1, activation='sigmoid', input_shape=(image_size,image_size,3), encoder_weights=encoder_weights) \n",
        "    elif md_[0] is 'pspnet':\n",
        "        model = sm.PSPNet(md_[1], classes=1, activation='sigmoid', input_shape=(image_size,image_size,3), encoder_weights=encoder_weights) \n",
        "    elif md_[0] is 'fpn':\n",
        "        model = sm.FPN(md_[1], classes=1, activation='sigmoid', input_shape=(image_size,image_size,3), encoder_weights=encoder_weights) \n",
        "    else: \n",
        "        model = sm.Unet(md_[1], classes=1, activation='sigmoid', input_shape=(image_size,image_size,3), encoder_weights=encoder_weights) \n",
        "    model.compile(optim, total_loss, metrics)\n",
        "    # auto set model name with model_path\n",
        "    model.name = model_path\n",
        "    return model, lr, epochs, image_size, pretrained, batch_size, metrics, total_loss, optim\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aowmVe7GEiLY",
        "colab_type": "text"
      },
      "source": [
        "#### Persiapan Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyWBEAuXEfBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "5d296c27-1c07-4a0f-cce4-ecc130d06d38"
      },
      "source": [
        "##########____________________________________________ Pengaturan Model dan Dataset ____________________________________________##########\n",
        "\n",
        "### Untuk mengambil model sm, menggunakan fungsi take_model() ###\n",
        "#model, lr, epochs, image_size, pretrained, batch_size, metrics, total_loss, optim = take_model(model_path)\n",
        "#train_gen, test_gen, valid_gen, train_steps, test_steps, valid_steps = set_data(train_jt_ids,test_jt_ids,valid_jt_ids)\n",
        "\n",
        "#model baru\n",
        "model, lr, epochs, image_size, pretrained, batch_size, metrics, total_loss, optim = take_sm(model_path)\n",
        "train_gen, test_gen, valid_gen, train_steps, test_steps, valid_steps = set_data(train_jt_ids,test_jt_ids,valid_jt_ids)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memulai SM-Model:  unet_vgg16_jt_0.001_100_256_yes\n",
            "lr: 0.001 , epochs: 100 , im_size: 256 , prt: yes\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hifggT38EiHN",
        "colab_type": "text"
      },
      "source": [
        "#### Fungsi Training Testing dan Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yEBaGy3EZoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import plot_model\n",
        "import json, codecs, pickle\n",
        "import pandas as pd\n",
        "\n",
        "# helper function for training visualization\n",
        "def train_vis(history, model_save_dir, doing=\"show\"):\n",
        "    # Plot training & validat accuray values\n",
        "    plt.figure(figsize=(30, 5))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(history['accuracy'])\n",
        "    plt.plot(history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(122)\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "    if (doing==\"save\"):\n",
        "        plt.savefig(model_save_dir+'_plot.png')\n",
        "        print(\"Success Saving Plot\")\n",
        "        plt.clf()\n",
        "    else: \n",
        "        plt.show()\n",
        "\n",
        "# helper function for training \n",
        "def train_fit(model_, epochs=epochs, pretrain=False):\n",
        "    print(\"Training for \"+model_.name)\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(save_dir[0]+\"{}/{}\".format(model_.name,model_.name)+'_best_weights.h5', monitor='val_iou_score', save_weights_only=True, save_best_only=True, mode='max'),\n",
        "    ]\n",
        "    model_.fit_generator( train_gen, \n",
        "                          validation_data=valid_gen, \n",
        "                          steps_per_epoch=train_steps, \n",
        "                          validation_steps=valid_steps, \n",
        "                          epochs=epochs,\n",
        "                          callbacks=callbacks)\n",
        "        \n",
        "    if pretrain:\n",
        "        return 0\n",
        "        \n",
        "    try:\n",
        "        os.mkdir(save_dir[0]+model_.name)\n",
        "    except FileExistsError:\n",
        "        print('Directory not created, '+model_.name+' was exist!')\n",
        "\n",
        "    model_save_dir = save_dir[0]+model_.name+'/'+model_.name\n",
        "\n",
        "    plot_model(model_, show_shapes=True, to_file=model_save_dir+'_architecture.png')\n",
        "\n",
        "    train_vis(model_.history.history, model_save_dir, doing=\"save\")\n",
        "\n",
        "    with open(model_save_dir+'_history.bin', 'wb') as handle:\n",
        "        pickle.dump(model_.history.history, handle)\n",
        "    \n",
        "    model_.save(model_save_dir+'.h5')\n",
        "    print(\"Success Saving Model\")\n",
        "\n",
        "# helper function for testing \n",
        "def test_eval(model_):\n",
        "    i_=0\n",
        "    list_of_test = []\n",
        "    print(\"Testing for \"+model_.name)\n",
        "    scores = model_.evaluate_generator(test_gen)\n",
        "    list_of_test.append(\"Loss: {:.5}\".format(scores[0]))\n",
        "    print(list_of_test[i_])\n",
        "    for metric, value in zip(metrics, scores[1:]):\n",
        "        i_ += 1\n",
        "        try:\n",
        "            list_of_test.append(\"mean {}: {:.5}\".format(metric.__name__, value))\n",
        "        except: \n",
        "            list_of_test.append(\"mean accuracy : {:.5}\".format(value))\n",
        "        print(list_of_test[i_])\n",
        "\n",
        "    model_save_dir = save_dir[0]+model_.name+'/'+model_.name\n",
        "    \n",
        "    with open(model_save_dir+'_scores.txt', 'w') as f:\n",
        "        for item in list_of_test:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# helper function for data visualization\n",
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "    \n",
        "# helper function for data visualization    \n",
        "def denormalize(x):\n",
        "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
        "    x_max = np.percentile(x, 98)\n",
        "    x_min = np.percentile(x, 2)    \n",
        "    x = (x - x_min) / (x_max - x_min)\n",
        "    x = x.clip(0, 1)\n",
        "    return x\n",
        "\n",
        "# helper function for test result and visualization\n",
        "def test_vis(model, test_ids=test_jt_ids, evals=False):\n",
        "    # new definition test data \n",
        "    test_dataset = DataGen(test_ids, train_path, image_size=image_size, batch_size=1)\n",
        "    \n",
        "    loss_ = []\n",
        "    ious_ = []\n",
        "    f1s_ = []\n",
        "    task_ = ''\n",
        "\n",
        "    if evals:\n",
        "        jml = len(test_dataset)\n",
        "        ids = range(0,jml)\n",
        "        task_ = 'Sukses mengevaluasi model '+model.name+', dengan jumlah data_test: '+str(jml)\n",
        "    else:\n",
        "        #n = 5\n",
        "        #ids = np.random.choice(np.arange(len(test_dataset)), size=n)\n",
        "        jml = len(test_dataset)\n",
        "        ids = range(0,jml)\n",
        "\n",
        "    # visualize\n",
        "    for i in ids:\n",
        "        image, gt_mask = test_dataset[i]\n",
        "        image = np.expand_dims(image[0], axis=0)\n",
        "        pr_mask = model.predict(image).round()\n",
        "      \n",
        "        if not evals:\n",
        "            visualize(\n",
        "                image=denormalize(image.squeeze()),\n",
        "                gt_mask=gt_mask[0][..., 0].squeeze(),\n",
        "                pr_mask=pr_mask[0][..., 0].squeeze(),\n",
        "            )\n",
        "        else:\n",
        "            scores = model.evaluate(image,gt_mask)\n",
        "            \n",
        "            if (i+1) % 10 == 0 :\n",
        "                print('['+str(i+1)+'/'+str(jml)+'] --- dari progres ---')\n",
        "            \n",
        "            loss_.append(scores[0])\n",
        "            ious_.append(scores[1])\n",
        "            f1s_.append(scores[2])\n",
        "\n",
        "    if evals:\n",
        "        df = pd.DataFrame({'id': test_ids[:jml], 'loss': loss_, 'iou-score': ious_, 'f1-score': f1s_})\n",
        "        df.to_csv(save_dir[0]+model.name+'/'+model.name+'_eval_test.csv',index=False)\n",
        "    print()\n",
        "    print(task_)\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmMQUk1JEtJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "910632c5-fe06-4a6e-cd7c-d50a4a326e07"
      },
      "source": [
        "round(0.1284*100,1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_NllGrQEz3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load weights in the best scores for each overall epoh\n",
        "# model.load_weights('/content/drive/My Drive/PENELITIAN/models/unet_mobilenet_17a_0.001_100_256_yes/unet_mobilenet_17a_0.001_100_256_yes_best_weights.h5')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w1kwsriE3V3",
        "colab_type": "text"
      },
      "source": [
        "#### Train The Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrGfgjn-E4zO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "outputId": "c028f5eb-c032-4da2-c1a6-10ec02ac8cc9"
      },
      "source": [
        "## Melanjutkan Training\n",
        "#cont_training(model)\n",
        "\n",
        "# Training Model Baru\n",
        "train_fit(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for unet_vgg16_jt_0.001_100_256_yes\n",
            "Epoch 1/100\n",
            "438/438 [==============================] - 2314s 5s/step - loss: 0.7265 - iou_score: 0.2919 - f1-score: 0.4234 - accuracy: 0.9630 - val_loss: 0.7530 - val_iou_score: 0.3070 - val_f1-score: 0.4607 - val_accuracy: 0.9847\n",
            "Epoch 2/100\n",
            "438/438 [==============================] - 187s 427ms/step - loss: 0.5494 - iou_score: 0.4563 - f1-score: 0.6050 - accuracy: 0.9906 - val_loss: 0.7459 - val_iou_score: 0.1835 - val_f1-score: 0.3060 - val_accuracy: 0.9412\n",
            "Epoch 3/100\n",
            "438/438 [==============================] - 184s 420ms/step - loss: 0.4373 - iou_score: 0.5669 - f1-score: 0.7040 - accuracy: 0.9933 - val_loss: 0.6141 - val_iou_score: 0.3505 - val_f1-score: 0.5104 - val_accuracy: 0.9812\n",
            "Epoch 4/100\n",
            "438/438 [==============================] - 184s 420ms/step - loss: 0.3799 - iou_score: 0.6234 - f1-score: 0.7537 - accuracy: 0.9947 - val_loss: 0.3068 - val_iou_score: 0.5540 - val_f1-score: 0.6954 - val_accuracy: 0.9929\n",
            "Epoch 5/100\n",
            "438/438 [==============================] - 184s 420ms/step - loss: 0.3121 - iou_score: 0.6903 - f1-score: 0.8057 - accuracy: 0.9958 - val_loss: 0.3553 - val_iou_score: 0.6015 - val_f1-score: 0.7447 - val_accuracy: 0.9955\n",
            "Epoch 6/100\n",
            "438/438 [==============================] - 185s 423ms/step - loss: 0.3126 - iou_score: 0.6894 - f1-score: 0.8037 - accuracy: 0.9958 - val_loss: 0.2659 - val_iou_score: 0.4646 - val_f1-score: 0.6063 - val_accuracy: 0.9922\n",
            "Epoch 7/100\n",
            "438/438 [==============================] - 186s 424ms/step - loss: 0.2795 - iou_score: 0.7223 - f1-score: 0.8304 - accuracy: 0.9963 - val_loss: 0.6353 - val_iou_score: 0.3345 - val_f1-score: 0.4355 - val_accuracy: 0.9896\n",
            "Epoch 8/100\n",
            "438/438 [==============================] - 185s 422ms/step - loss: 0.2596 - iou_score: 0.7420 - f1-score: 0.8455 - accuracy: 0.9967 - val_loss: 0.2521 - val_iou_score: 0.6657 - val_f1-score: 0.7905 - val_accuracy: 0.9949\n",
            "Epoch 9/100\n",
            "438/438 [==============================] - 185s 422ms/step - loss: 0.2617 - iou_score: 0.7395 - f1-score: 0.8430 - accuracy: 0.9966 - val_loss: 0.2978 - val_iou_score: 0.6254 - val_f1-score: 0.7496 - val_accuracy: 0.9965\n",
            "Epoch 10/100\n",
            "438/438 [==============================] - 185s 423ms/step - loss: 0.2459 - iou_score: 0.7552 - f1-score: 0.8543 - accuracy: 0.9969 - val_loss: 0.1997 - val_iou_score: 0.5535 - val_f1-score: 0.6910 - val_accuracy: 0.9934\n",
            "Epoch 11/100\n",
            "438/438 [==============================] - 185s 423ms/step - loss: 0.2369 - iou_score: 0.7642 - f1-score: 0.8614 - accuracy: 0.9970 - val_loss: 0.7171 - val_iou_score: 0.4863 - val_f1-score: 0.6111 - val_accuracy: 0.9920\n",
            "Epoch 12/100\n",
            "438/438 [==============================] - 185s 422ms/step - loss: 0.2293 - iou_score: 0.7717 - f1-score: 0.8665 - accuracy: 0.9972 - val_loss: 0.2053 - val_iou_score: 0.6640 - val_f1-score: 0.7881 - val_accuracy: 0.9949\n",
            "Epoch 13/100\n",
            "438/438 [==============================] - 185s 422ms/step - loss: 0.2318 - iou_score: 0.7690 - f1-score: 0.8644 - accuracy: 0.9970 - val_loss: 0.3525 - val_iou_score: 0.6270 - val_f1-score: 0.7645 - val_accuracy: 0.9950\n",
            "Epoch 14/100\n",
            "438/438 [==============================] - 185s 423ms/step - loss: 0.2252 - iou_score: 0.7757 - f1-score: 0.8696 - accuracy: 0.9972 - val_loss: 0.2255 - val_iou_score: 0.6590 - val_f1-score: 0.7867 - val_accuracy: 0.9949\n",
            "Epoch 15/100\n",
            "438/438 [==============================] - 185s 422ms/step - loss: 0.2250 - iou_score: 0.7758 - f1-score: 0.8691 - accuracy: 0.9972 - val_loss: 0.4566 - val_iou_score: 0.6422 - val_f1-score: 0.7720 - val_accuracy: 0.9941\n",
            "Epoch 16/100\n",
            "438/438 [==============================] - 185s 422ms/step - loss: 0.2160 - iou_score: 0.7848 - f1-score: 0.8752 - accuracy: 0.9973 - val_loss: 0.1751 - val_iou_score: 0.6222 - val_f1-score: 0.7478 - val_accuracy: 0.9943\n",
            "Epoch 17/100\n",
            "438/438 [==============================] - 185s 422ms/step - loss: 0.2096 - iou_score: 0.7911 - f1-score: 0.8802 - accuracy: 0.9975 - val_loss: 0.4887 - val_iou_score: 0.5979 - val_f1-score: 0.7434 - val_accuracy: 0.9945\n",
            "Epoch 18/100\n",
            "438/438 [==============================] - 185s 422ms/step - loss: 0.2078 - iou_score: 0.7928 - f1-score: 0.8813 - accuracy: 0.9975 - val_loss: 0.2378 - val_iou_score: 0.6697 - val_f1-score: 0.7940 - val_accuracy: 0.9938\n",
            "Epoch 19/100\n",
            "438/438 [==============================] - 185s 423ms/step - loss: 0.2087 - iou_score: 0.7919 - f1-score: 0.8804 - accuracy: 0.9974 - val_loss: 0.5321 - val_iou_score: 0.6381 - val_f1-score: 0.7646 - val_accuracy: 0.9942\n",
            "Epoch 20/100\n",
            "438/438 [==============================] - 185s 422ms/step - loss: 0.2065 - iou_score: 0.7941 - f1-score: 0.8818 - accuracy: 0.9974 - val_loss: 0.2042 - val_iou_score: 0.6455 - val_f1-score: 0.7778 - val_accuracy: 0.9945\n",
            "Epoch 21/100\n",
            "316/438 [====================>.........] - ETA: 48s - loss: 0.1980 - iou_score: 0.8026 - f1-score: 0.8880 - accuracy: 0.9976"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6D_A7OhE7HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = keras.models.load_model('/content/drive/My Drive/SKRIPSI/models/unet_mobilenet_jt_0.001_15_256_yes/unet_mobilenet_jt_0.001_15_256_yes.h5',custom_objects={'jaccard_loss': total_loss, 'iou_score':metrics[0], 'f1-score':metrics[1]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C8x4l7nE9PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights('/content/drive/My Drive/SKRIPSI/models/unet_mobilenet_jt_0.001_15_256_yes/unet_mobilenet_jt_0.001_15_256_yes.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECywxCDBE_9o",
        "colab_type": "text"
      },
      "source": [
        "#### Test The Models \n",
        "`*(Tidak perlu dijalankan jika masih checkpoint)*`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCoSVSf3E9NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_eval(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ZsB9CJE7KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_vis(model, test_ids=test_jt_ids, evals=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GT4-1-L_a94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1rE7F94FGq_",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluasi Gambar\n",
        "``Print kondisi setiap channel pada setiap layer untuk gambar yang akan didefinisikan``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SFN4teTFJWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_csvs = save_dir[0]+model.name+\"/\"+model.name+\"_eval_test.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCuAikiYFMHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.read_csv(load_csvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1MKT3sHFOmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfcsv = pd.read_csv(load_csvs).sort_values('iou-score')\n",
        "nilai_iou = []\n",
        "# dfcsv\n",
        "for niou in dfcsv['iou-score']:\n",
        "     nilai_iou.append(int(round(niou*100,0)))\n",
        "nilai_iou[0] = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiLLT3taFQel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_ = list(pd.DataFrame(nilai_iou)[0].value_counts().index)\n",
        "values_ = list(pd.DataFrame(nilai_iou)[0].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsGTlwJCFQl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = index_\n",
        "values = values_\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.bar(names, values)\n",
        "plt.xticks(list(range(0,101,2)))\n",
        "plt.yticks(list(range(0,31,2)))\n",
        "plt.xlabel('Skor IoU (Pembulatan 0 angka di belakang koma)')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.grid()\n",
        "plt.suptitle('Sebaran Skor IoU pada 500 Data Uji')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtFgmU0_FMPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = index_\n",
        "values = values_\n",
        "\n",
        "plt.figure(figsize=(3, 15))\n",
        "\n",
        "plt.boxplot(nilai_iou)\n",
        "plt.ylabel('Skor IoU (%)')\n",
        "plt.yticks(list(range(0,101,2)))\n",
        "plt.grid()\n",
        "plt.suptitle('Sebaran Skor IoU pada 600 Data Uji')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edZNfgInFUsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfcsv = pd.read_csv(load_csvs).sort_values('iou-score')\n",
        "print(dfcsv[dfcsv['iou-score']<0.1].count()[0])\n",
        "print(dfcsv[dfcsv['iou-score']>=0.1][dfcsv['iou-score']<=0.5].count()[0])\n",
        "print(dfcsv[dfcsv['iou-score']>0.5][dfcsv['iou-score']<=0.85].count()[0])\n",
        "print(dfcsv[dfcsv['iou-score']>0.85][dfcsv['iou-score']<=0.95].count()[0])\n",
        "print(dfcsv[dfcsv['iou-score']>0.95].count()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-U0Eyb5FXFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.read_csv(load_csvs)['iou-score'].sum()/600"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-Mc_EwyFZj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Hasil ini digunakan pada model yang dilatih dengan batch_size 8')\n",
        "print()\n",
        "test_dataset = DataGen(test_jt_ids, train_path, image_size=image_size, batch_size=8)\n",
        "hasils = model.evaluate_generator(test_dataset)\n",
        "print(\"Hasil pengujian dengan batch_size = 8 (sesuai ukuran pelatihan)\")\n",
        "print(\"Loss: \",hasils[0])\n",
        "print(\"IoU-Score: \",hasils[1])\n",
        "print(\"F1-Score: \",hasils[2])\n",
        "print(\"accuracy: \",hasils[3])\n",
        "print()\n",
        "\n",
        "test_dataset = DataGen(test_jt_ids, train_path, image_size=image_size, batch_size=1)\n",
        "hasils = model.evaluate_generator(test_dataset)\n",
        "print(\"Hasil pengujian dengan batch_size = 1 (satu-per-satu gambar)\")\n",
        "print(\"Loss: \",hasils[0])\n",
        "print(\"IoU-Score: \",hasils[1])\n",
        "print(\"F1-Score: \",hasils[2])\n",
        "print(\"accuracy: \",hasils[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFlQPlQ6FfWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfcsv = pd.read_csv(load_csvs).sort_values('iou-score')\n",
        "test_id1 = list(dfcsv['id'][dfcsv['iou-score']<0.1])\n",
        "test_id2 = list(dfcsv['id'][dfcsv['iou-score']>=0.1][dfcsv['iou-score']<=0.5])\n",
        "test_id3 = list(dfcsv['id'][dfcsv['iou-score']>0.5][dfcsv['iou-score']<=0.85])\n",
        "test_id4 = list(dfcsv['id'][dfcsv['iou-score']>0.85][dfcsv['iou-score']<=0.95])\n",
        "test_id5 = list(dfcsv['id'][dfcsv['iou-score']>0.95])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uyQWnUKFiW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fungsi_vis(i,test_id1):\n",
        "  test_dataset = DataGen(test_id1, train_path, image_size=image_size, batch_size=1)\n",
        "  plt.figure(figsize=(16, 5))\n",
        "  image, gt_mask = test_dataset[i]\n",
        "  image = np.expand_dims(image[0], axis=0)\n",
        "  pr_mask = model.predict(image).round()\n",
        "  scores = model.evaluate(image,gt_mask)\n",
        "  b,g,r = cv2.split(image[0])\n",
        "  rgb_img = cv2.merge([r,g,b])\n",
        "  # imagess = np.expand_dims(rgb_img, axis=0)\n",
        "\n",
        "  contours1, hierarchy1 = cv2.findContours(np.array(gt_mask[0], dtype=np.uint8).squeeze(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
        "  largest_contour1 = []\n",
        "  largest_area = 0\n",
        "  for contour in contours1:\n",
        "      area = cv2.contourArea(contour)\n",
        "      if area > largest_area:\n",
        "          largest_area = area\n",
        "          largest_contour1 = contour\n",
        "  cv2.drawContours(rgb_img, [largest_contour1], -2, (1, 1, 0), 1)\n",
        "\n",
        "  contours2, hierarchy2 = cv2.findContours(np.array(pr_mask, dtype=np.uint8).squeeze(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
        "  largest_contour2 = []\n",
        "  largest_area = 0\n",
        "  for contour in contours2:\n",
        "      area = cv2.contourArea(contour)\n",
        "      if area > largest_area:\n",
        "          largest_area = area\n",
        "          largest_contour2 = contour\n",
        "  cv2.drawContours(rgb_img, [largest_contour2], -1, (0, 1, 1), 1)\n",
        "  ious = str(scores[1]*100)[:str(scores[1]*100).find(\".\")+3] + '%'\n",
        "  titles = test_id1[i]+\" (\"+ious+\")\"\n",
        "  plt.title(titles)\n",
        "  plt.imshow(denormalize(rgb_img.squeeze()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnxp_he8FmMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(['sol_002_z_pos_011_t_pos_006','sol_001_z_pos_002_t_pos_001','sol_001_z_pos_002_t_pos_001'])):\n",
        "  fungsi_vis(i,['sol_002_z_pos_011_t_pos_006','sol_001_z_pos_002_t_pos_001','sol_001_z_pos_002_t_pos_001'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4bzRrh6Fj8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_id3[50:100])):\n",
        "  fungsi_vis(i,test_id3[50:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNOScWPUFohC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_id3[-70:-20])):\n",
        "  fungsi_vis(i,test_id3[-70:-20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaEncNP_FqEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_id4[:20])):\n",
        "  fungsi_vis(i,test_id4[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpa3BQWeFqLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_id4[-120:-80])):\n",
        "  fungsi_vis(i,test_id4[-120:-80])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-bQIxyVFtgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_id5)):\n",
        "  fungsi_vis(i,test_id5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8F7OoncFwSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########____________________________________________________________________________##########\n",
        "\n",
        "# Gambar yang akan dievaluasi (Definisikan!!!)\n",
        "jenisimg = ['own','97','90','85','70','50','20','0,01','0,001'] # Kategori Berdasarkan Mean-IoU-Scores\n",
        "eval_ids = ['sol_001_z_pos_002_t_pos_001','sol_001_z_pos_002_t_pos_015','sol_001_z_pos_002_t_pos_016','sol_001_z_pos_002_t_pos_017','sol_001_z_pos_002_t_pos_019','sol_001_z_pos_002_t_pos_020','sol_001_z_pos_003_t_pos_002','sol_001_z_pos_003_t_pos_003','sol_001_z_pos_003_t_pos_004']\n",
        "\n",
        "link_fldr = '/content/drive/My Drive/SKRIPSI/visualisasi/'\n",
        "\n",
        "##########____________________________________________________________________________##########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVjDPD19FodR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_data = DataGen(eval_ids, train_path, image_size=image_size, batch_size=1)\n",
        "x,y = eval_data.__getitem__(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVAQa0s4_Vgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X6f34J8F0mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "# Print Image Input Layers\n",
        "def printInput(x,y,i):\n",
        "    print(x[0].shape)\n",
        "    b,g,r = cv2.split(x[0])\n",
        "    rgb_img = cv2.merge([r,g,b]) \n",
        "    pr = model.predict(x)\n",
        "    fig, axs = plt.subplots(1, 6, figsize=(30,5))\n",
        "    fig.suptitle('(0) Layer Input')\n",
        "    axs[0].imshow(rgb_img) # Image\n",
        "    axs[0].set_title('Image '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "    axs[1].imshow(x[0][:,:,0]) # Blue\n",
        "    axs[1].set_title('Blue channel '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "    axs[2].imshow(x[0][:,:,1]) # Green\n",
        "    axs[2].set_title('Green channel '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "    axs[3].imshow(x[0][:,:,2]) # Red\n",
        "    axs[3].set_title('Red channel '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "    axs[4].imshow(y[0][:,:,0]) # Mask\n",
        "    axs[4].set_title('Mask '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "    axs[5].imshow(pr[0][:,:,0]) # Predict\n",
        "    axs[5].set_title('Predict '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "\n",
        "    fig.savefig(link_fldr+eval_ids[i]+'/(0) Layer Input.png')\n",
        "    print(\"Success save layer input...\")\n",
        "\n",
        "# Pembagi Plot Size \n",
        "def rec(size_,pem=2,pen=2):\n",
        "    hs = size_/4\n",
        "    if size_ == 1: return pem-pem, pen-pen\n",
        "    if hs == 1: return pem, pen\n",
        "    elif hs % 4 != 0: return pem, pen*2\n",
        "    else: return rec(hs,pem*2,pen*2)\n",
        "\n",
        "# Print Image Every Layers\n",
        "def printLayer(model,x,k,first_,last_):\n",
        "    for i in range(first_,last_):\n",
        "        print('('+str(i)+') Layer '+model.layers[i].name+'.png')\n",
        "        if 'concat' in model.layers[i].name or 'pad' in model.layers[i].name:\n",
        "            print('Skip layer '+model.layers[i].name)\n",
        "        else:\n",
        "            get_output = K.function([model.layers[0].input],\n",
        "                                    [model.layers[i].output])\n",
        "            \n",
        "            layer_output = get_output([x])[0]\n",
        "\n",
        "            pem, pen = rec(len(layer_output[0][0][0]))\n",
        "\n",
        "            if pem == 0:\n",
        "                plt.imshow(layer_output[0][...,0])\n",
        "                plt.title(\"Ch_1\")\n",
        "                # Save Subplot.\n",
        "                plt.savefig(link_fldr+eval_ids[k]+'/('+str(i)+') Layer '+model.layers[i].name+'.png')\n",
        "            else:\n",
        "\n",
        "                # Create a Subplot.\n",
        "                fig, axs = plt.subplots(pen, pem, figsize=(32,32*pen//pem))\n",
        "                fig.suptitle('('+str(i)+') Layer '+model.layers[i].name)\n",
        "                \n",
        "                for ii in range(pen):\n",
        "                    for j in range(pem):\n",
        "                        axs[ii, j].imshow(layer_output[0][...,(ii*pem)+j])\n",
        "                        axs[ii, j].axis('off')\n",
        "                        axs[ii, j].set_title(\"Ch_\"+str((ii*pem)+j+1))\n",
        "                \n",
        "                # Save Subplot.\n",
        "                fig.savefig(link_fldr+eval_ids[k]+'/('+str(i)+') Layer '+model.layers[i].name+'.png')\n",
        "\n",
        "            print(\"Success save layer \"+model.layers[i].name+\"...\")\n",
        "            print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXyfa40EFzA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definisikan lt berdasarkan urutan gambar (dibagi supaya ram tidak penuh)\n",
        "lt = 0\n",
        "\n",
        "print('Percobaan kurang:',len(eval_ids)-lt-1,'gambar')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjz-VSj8F2ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4482caa4-b374-4fef-d787-4438106114b5"
      },
      "source": [
        "# Execute 1/2 data\n",
        "for i in range(lt,lt+1):\n",
        "    print(eval_ids[i])\n",
        "    try:\n",
        "        os.mkdir(link_fldr+eval_ids[i]+'/')\n",
        "    except FileExistsError:\n",
        "        print('Directory not created, '+eval_ids[i]+' was exist!')\n",
        "    x_,y_ = eval_data.__getitem__(i)\n",
        "    printInput(x_,y_,i)\n",
        "    printLayer(model,x_,i,1,len(model.layers)//2)\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sol_001_z_pos_002_t_pos_001\n",
            "Directory not created, sol_001_z_pos_002_t_pos_001 was exist!\n",
            "(256, 256, 3)\n",
            "Success save layer input...\n",
            "(1) Layer conv1_pad.png\n",
            "Skip layer conv1_pad\n",
            "(2) Layer conv1.png\n",
            "Success save layer conv1...\n",
            "\n",
            "(3) Layer conv1_bn.png\n",
            "Success save layer conv1_bn...\n",
            "\n",
            "(4) Layer conv1_relu.png\n",
            "Success save layer conv1_relu...\n",
            "\n",
            "(5) Layer conv_dw_1.png\n",
            "Success save layer conv_dw_1...\n",
            "\n",
            "(6) Layer conv_dw_1_bn.png\n",
            "Success save layer conv_dw_1_bn...\n",
            "\n",
            "(7) Layer conv_dw_1_relu.png\n",
            "Success save layer conv_dw_1_relu...\n",
            "\n",
            "(8) Layer conv_pw_1.png\n",
            "Success save layer conv_pw_1...\n",
            "\n",
            "(9) Layer conv_pw_1_bn.png\n",
            "Success save layer conv_pw_1_bn...\n",
            "\n",
            "(10) Layer conv_pw_1_relu.png\n",
            "Success save layer conv_pw_1_relu...\n",
            "\n",
            "(11) Layer conv_pad_2.png\n",
            "Skip layer conv_pad_2\n",
            "(12) Layer conv_dw_2.png\n",
            "Success save layer conv_dw_2...\n",
            "\n",
            "(13) Layer conv_dw_2_bn.png\n",
            "Success save layer conv_dw_2_bn...\n",
            "\n",
            "(14) Layer conv_dw_2_relu.png\n",
            "Success save layer conv_dw_2_relu...\n",
            "\n",
            "(15) Layer conv_pw_2.png\n",
            "Success save layer conv_pw_2...\n",
            "\n",
            "(16) Layer conv_pw_2_bn.png\n",
            "Success save layer conv_pw_2_bn...\n",
            "\n",
            "(17) Layer conv_pw_2_relu.png\n",
            "Success save layer conv_pw_2_relu...\n",
            "\n",
            "(18) Layer conv_dw_3.png\n",
            "Success save layer conv_dw_3...\n",
            "\n",
            "(19) Layer conv_dw_3_bn.png\n",
            "Success save layer conv_dw_3_bn...\n",
            "\n",
            "(20) Layer conv_dw_3_relu.png\n",
            "Success save layer conv_dw_3_relu...\n",
            "\n",
            "(21) Layer conv_pw_3.png\n",
            "Success save layer conv_pw_3...\n",
            "\n",
            "(22) Layer conv_pw_3_bn.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Success save layer conv_pw_3_bn...\n",
            "\n",
            "(23) Layer conv_pw_3_relu.png\n",
            "Success save layer conv_pw_3_relu...\n",
            "\n",
            "(24) Layer conv_pad_4.png\n",
            "Skip layer conv_pad_4\n",
            "(25) Layer conv_dw_4.png\n",
            "Success save layer conv_dw_4...\n",
            "\n",
            "(26) Layer conv_dw_4_bn.png\n",
            "Success save layer conv_dw_4_bn...\n",
            "\n",
            "(27) Layer conv_dw_4_relu.png\n",
            "Success save layer conv_dw_4_relu...\n",
            "\n",
            "(28) Layer conv_pw_4.png\n",
            "Success save layer conv_pw_4...\n",
            "\n",
            "(29) Layer conv_pw_4_bn.png\n",
            "Success save layer conv_pw_4_bn...\n",
            "\n",
            "(30) Layer conv_pw_4_relu.png\n",
            "Success save layer conv_pw_4_relu...\n",
            "\n",
            "(31) Layer conv_dw_5.png\n",
            "Success save layer conv_dw_5...\n",
            "\n",
            "(32) Layer conv_dw_5_bn.png\n",
            "Success save layer conv_dw_5_bn...\n",
            "\n",
            "(33) Layer conv_dw_5_relu.png\n",
            "Success save layer conv_dw_5_relu...\n",
            "\n",
            "(34) Layer conv_pw_5.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YiiBW1ZF6Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Execute 2/2 data\n",
        "for i in range(lt,lt+1):\n",
        "   print(eval_ids[i])\n",
        "    try:\n",
        "        os.mkdir(link_fldr+eval_ids[i]+'/')\n",
        "    except FileExistsError:\n",
        "        print('Directory not created, '+eval_ids[i]+' was exist!')\n",
        "    x_,y_ = eval_data.__getitem__(i)\n",
        "    printInput(x_,y_,i)\n",
        "    printLayer(model,x_,i,len(model.layers)//2,len(model.layers))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow7kyJVhF9Fq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#### Finish !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRp82VzGGCC2",
        "colab_type": "text"
      },
      "source": [
        "![alt text]Finish !... Semua File Akan Otomatis Replace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-w8mZYiGFVE",
        "colab_type": "text"
      },
      "source": [
        "**Penting !...** *``Cara memastikan model sudah 500 epochs yaitu dengan melihat \n",
        "\n",
        "---\n",
        "\n",
        "size pada file \"history.bin\", ketika sudah berukuran **\"91KB\"** artinya model tersebut sudah 500 epochs...*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb01g63PF2tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6wqKWJQKUoA",
        "colab_type": "text"
      },
      "source": [
        "###Coba coba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTXROCWtF9nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#############___________________#############\n",
        "\n",
        "models_name = [\n",
        "               'unet_mobilenet_17a_0.001_100_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_10_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_50_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_100_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_500_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_15_256_yes',\n",
        "]\n",
        "\n",
        "list_models = []\n",
        "list_iousbest = []\n",
        "\n",
        "\n",
        "\n",
        "for name in models_name:\n",
        "    list_models.append(name)\n",
        "    model.load_weights(save_dir[0]+name+'/'+name+'_best_weights.h5')\n",
        "    print(model.name+'_bestweight')\n",
        "    testing = test_eval(model)\n",
        "    list_iousbest.append(testing[1]) # tambahin ini dis\n",
        "\n",
        "import pandas as pd\n",
        "print(len(list_models))\n",
        "print(len(list_iousbest))\n",
        "pd.DataFrame({'model':list_models, 'test_best': list_iousbest}).to_csv('balancing_model_this.xls', index=False)\n",
        "\n",
        "#_______ UNTUK TEST_EVAL SEMUA MODEL _______# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHXBQq5BLUeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#############___________________#############\n",
        "\n",
        "models_name = [\n",
        "               'unet_mobilenet_17a_0.001_100_256_yes', \n",
        "               'unet_mobilenet_jt_0.001_500_256_yes', \n",
        "               'unet_mobilenet_jt_0.001_50_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_15_256_yes' \n",
        "]\n",
        "\n",
        "list_models = []\n",
        "list_iousbest = []\n",
        "\n",
        "\n",
        "for name in models_name:\n",
        "    list_models.append(name)\n",
        "    model.load_weights(save_dir[0]+name+'/'+name+'_best_weights.h5')\n",
        "    print(model.name+'_bestweight')\n",
        "    testing = test_eval(model)\n",
        "    list_iousbest.append(testing[1])\n",
        "\n",
        "\n",
        "# import pandas as pd\n",
        "# pd.DataFrame({'model':list_models, 'test_best': list_iousbest, 'test_last': list_iouslast}).to_csv('balancing_model_this.xls', index=False)\n",
        "\n",
        "#_______ UNTUK TEST_EVAL SEMUA MODEL _______# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4vSDQzDLWm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for namee in sorted(os.listdir('bin/')):\n",
        "    if '192' in namee:\n",
        "        histoo = loadHist('bin/'+namee)\n",
        "        print(namee, histoo['val_iou_score'].index(max(histoo['val_iou_score']))+1)\n",
        "##cara buat file bin gimana? atau buat filenya sendiri?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUHIXVyHLai9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ptni-d5LeQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i_ in range(len(list_models)):\n",
        "#     image_size = int(list_models[i_].name.split('_')[5])\n",
        "#     train_gen, test_gen, valid_gen, train_steps, test_steps, valid_steps = set_data(train_17_ids,test_17_ids,valid_17_ids)\n",
        "#     test_vis(list_models[i_], test_ids=test_17_ids, evals=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3366TsAzLf5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y = test_gen.__getitem__(0)\n",
        "\n",
        "# OpenCV does not use RGB, it uses BGR (standing for blue, green, red). You need to swap the order of the red and the blue.\n",
        "b,g,r = cv2.split(x[0])           # get b, g, r\n",
        "rgb_img = cv2.merge([r,g,b])     # switch it to r, g, b\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12,12))\n",
        "\n",
        "axs[0, 0].imshow(x[0].squeeze())\n",
        "axs[0, 0].set_title('bgr image')\n",
        "\n",
        "axs[0, 1].imshow(denormalize(x[0]))\n",
        "axs[0, 1].set_title('bgr image denorm')\n",
        "\n",
        "axs[1, 0].imshow(rgb_img)\n",
        "axs[1, 0].set_title('rgb image')\n",
        "\n",
        "axs[1, 1].imshow(denormalize(rgb_img))\n",
        "axs[1, 1].set_title('rgb image denorm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95xQ9Zj-LjL2",
        "colab_type": "text"
      },
      "source": [
        "**Percobaaan pengambilan epochs terbaik**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCEnzasbLolQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "list_hist = {}\n",
        "best_models = {} \n",
        "\n",
        "for dirs in os.listdir(save_dir[0]):\n",
        "    if '.h5' in dirs or 'pretrained' in dirs or 'sm' in dirs or 'linknet' in dirs or '0.001_500_128_yes' in dirs:\n",
        "        continue\n",
        "    else:\n",
        "        path_dir = save_dir[0]+dirs+'/'\n",
        "        for dir_file in os.listdir(path_dir):\n",
        "            if 'history.bin' in dir_file:\n",
        "                list_hist[dirs] = loadHist(path_dir+dirs+'_history.bin')\n",
        "                val_i = list_hist[dirs]['val_iou_score']\n",
        "                maxim = 0\n",
        "                for ep in range(len(val_i)):\n",
        "                    if maxim < val_i[ep]:\n",
        "                        maxim = val_i[ep]\n",
        "                        dirs = dirs.replace('_100_','_500_')\n",
        "                        best_models[dirs] = (maxim,ep+1)\n",
        "\n",
        "\n",
        "bsms = [bsm for bsm in best_models]\n",
        "epochsm = [best_models[bsm][1] for bsm in best_models]\n",
        "valsm = [best_models[bsm][0] for bsm in best_models]\n",
        "df_epochs = pd.DataFrame({'model':bsms, 'epochs': epochsm, 'val_score': valsm})\n",
        "df_epochs.sort_values(ascending=False,axis=0,by='val_score').reset_index().drop(['index'],axis=1).to_csv('epochs_best_test.xls')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRXQnXoYLqZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_models = {} \n",
        "\n",
        "for dirs in os.listdir(save_dir[0]):\n",
        "    # if '.h5' in dirs or 'pretrained' in dirs or 'sm' in dirs or 'linknet' in dirs or '0.001_500_128_yes' in dirs:\n",
        "    #     continue\n",
        "    # else:\n",
        "        path_dir = save_dir[0]+dirs+'/'\n",
        "        for dir_file in os.listdir(path_dir):\n",
        "            if 'history.bin' in dir_file:\n",
        "                val_i = loadHist(path_dir+dirs+'_history.bin')['val_iou_score']\n",
        "            if 'scores.txt' in dir_file:\n",
        "                test_i = open(path_dir+dirs+'_scores.txt', 'r').read().split()\n",
        "                dirs = dirs.replace('_100_','_500_')\n",
        "                last_models[dirs] = (val_i[-1],test_i[4])\n",
        "\n",
        "\n",
        "bsms = [bsm for bsm in last_models]\n",
        "testsm = [last_models[bsm][1] for bsm in last_models]\n",
        "valsm = [last_models[bsm][0] for bsm in last_models]\n",
        "df_epochs = pd.DataFrame({'model':bsms, 'val_score': valsm, 'test_score': testsm})\n",
        "df_epochs.sort_values(ascending=False,axis=0,by='test_score').reset_index().drop(['index'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19X8AexbLsHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_data = DataGen(eval_ids, train_path, image_size=128, batch_size=1)\n",
        "x,y = eval_data.__getitem__(0)\n",
        "b,g,r = cv2.split(x[0])\n",
        "rgb_img = cv2.merge([r,g,b]) \n",
        "pr = model.predict(x)\n",
        "fig, axs = plt.subplots(1, 6, figsize=(30,5))\n",
        "axs[0].imshow(rgb_img) # Image\n",
        "axs[1].imshow(x[0][:,:,0], cmap='gray') # Blue\n",
        "axs[2].imshow(x[0][:,:,1], cmap='gray') # Green\n",
        "axs[3].imshow(x[0][:,:,2], cmap='gray') # Red\n",
        "axs[4].imshow(y[0][:,:,0], cmap='gray') # Mask\n",
        "axs[5].imshow(pr[0][:,:,0], cmap='gray') # Predict\n",
        "\n",
        "list_cmap = ['Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn,' 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'twilight', 'twilight_r', 'twilight_shifted']\n",
        "fig, axs = plt.subplots(1, len(list_cmap), figsize=(240,5))\n",
        "for i in range(len(list_cmap)):\n",
        "  try:\n",
        "    axs[i].imshow(x[0][:,:,0], cmap=list_cmap[i])\n",
        "    axs[i].axis('off')\n",
        "    axs[i].set_title(list_cmap[i])\n",
        "  except:\n",
        "    print(list_cmap[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulS1OefaLv0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#segmentation_models.metrics padahal ada dice-nya tapi ga ke load kenapa?\n",
        "model, lr, epochs, image_size, pretrained, batch_size, metrics, total_loss, optim = take_model('unet_mobilenet_jt_0.001_15_256_yes')\n",
        "model.save_weights(save_dir[0]+'unet_mobilenet_jt_0.001_15_256_yes.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f_mc_qULxrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gambar_sample = ['sol_001_z_pos_002_t_pos_001','sol_001_z_pos_002_t_pos_015','sol_001_z_pos_002_t_pos_016','sol_001_z_pos_002_t_pos_017','sol_001_z_pos_002_t_pos_019','sol_001_z_pos_002_t_pos_020','sol_001_z_pos_003_t_pos_002','sol_001_z_pos_003_t_pos_003','sol_001_z_pos_003_t_pos_004']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMo6NLLGLx4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#kalo ga ada 128 gimana?\n",
        "list_model_128 = [\n",
        "             'unet_mobilenet_17a_0.001_100_256_yes',\n",
        "               'unet_mobilenet_17a_0.001_100_256_no',\n",
        "               'unet_mobilenet_jt_0.001_10_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_10_256_no',\n",
        "               'unet_mobilenet_jt_0.001_50_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_50_256_no',\n",
        "               'unet_mobilenet_jt_0.001_100_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_100_256_no',\n",
        "               'unet_mobilenet_jt_0.001_500_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_500_256_no',\n",
        "               'unet_mobilenet_jt_0.001_15_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_15_256_no'\n",
        "]\n",
        "\n",
        "model, lr, epochs, image_size, pretrained, batch_size, metrics, total_loss, optim = take_model('unet_mobilenet_jt_0.001_15_256_yes')\n",
        "\n",
        "# for m in list_model_128:\n",
        "#     train_gen, test_gen, valid_gen, train_steps, test_steps, valid_steps = set_data(train_17_ids,test_17_ids,valid_17_ids)\n",
        "#     print()\n",
        "#     print('Loading weight...')\n",
        "#     model.name = m\n",
        "#     model.load_weights(save_dir[0]+m+'.h5')\n",
        "#     test_vis(model, test_ids=test_17_ids, evals=False, id_=gambar_sample)\n",
        "\n",
        "# print grafik\n",
        "for m in list_model_128:\n",
        "    print('Loading weight...')\n",
        "    model.load_weights(save_dir[0]+m+'.h5')\n",
        "    train_vis(model.history.history, save_dir[0]+m+'/'+m, doing=\"save\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtvYgFT3L0n2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_model_256 = [\n",
        "               'unet_mobilenet_17a_0.001_100_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_10_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_50_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_100_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_500_256_yes',\n",
        "               'unet_mobilenet_jt_0.001_15_256_yes',\n",
        "]\n",
        "\n",
        "model, lr, epochs, image_size, pretrained, batch_size, metrics, total_loss, optim = take_model('unet_mobilenet_17a_0.001_100_128_yes')\n",
        "\n",
        "for m in list_model_256:\n",
        "    train_gen, test_gen, valid_gen, train_steps, test_steps, valid_steps = set_data(train_jt_ids,test_jt_ids,valid_jt_ids)\n",
        "    print()\n",
        "    print('Loading weight...')\n",
        "    model.name = m\n",
        "    model.load_weights(save_dir[0]+m+'.h5')\n",
        "    test_vis(model, test_ids=test_17_ids, evals=False, id_=gambar_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6nFZ4mmL4sW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install patool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGLMA5qAL52F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#doesn't need at all\n",
        "#import patoolib\n",
        "#patoolib.create_archive('/content/drive/My Drive/vis.zip', ('/content/filevis',), program=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fav-wdtnhx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSqUgY2YuTFO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}